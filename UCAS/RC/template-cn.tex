%!TEX program = xelatex
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage[UTF8]{ctex}
\usepackage{fancyhdr}
\usepackage{cite}
\usepackage{epsfig}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{textcomp}
\usepackage{tabularx}
\usepackage{verbatim}
\usepackage{graphics,graphicx}
\usepackage{algorithm,algpseudocode}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{bm}
\usepackage{cases}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{amsmath,empheq,eqparbox}
\usepackage[perpage,symbol,flushmargin]{footmisc}

\renewcommand*\footnoterule{\hrule width 2.5cm height 0.4pt \vspace*{0.5ex}}

\newcommand{\eqmath}[3][c]{%
	% #1 = alignment, default c, #2 = label, #2 = math material
	\eqmakebox[#2][#1]{$\displaystyle#3$}%
}
\newcommand{\eqtext}[3][c]{%
	% #1 = alignment, default c, #2 = label, #2 = text material
	\eqmakebox[#2][#1]{#3}%
}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\fancypagestyle{mystyle}{%
	\fancyhf{} % 清空页眉和页脚的设置
	\fancyhead[LE,RO]{\thepage} % 在偶数页左侧和奇数页右侧显示页码
	\fancyhead[RE]{\leftmark} % 在偶数页右侧显示当前章的标题
	\fancyhead[LO]{\rightmark} % 在奇数页左侧显示当前节的标题
	\renewcommand{\headrulewidth}{0.4pt} % 设置页眉下方横线的粗细为0.4pt
	\renewcommand{\footrulewidth}{0pt} % 页脚上方无横线
}

\begin{document}
	
\pagestyle{mystyle}

\title{ELM原理及其在数据预测中的简单应用}

\author{
    \IEEEauthorblockN{
       宋超$^{\dagger, \ddagger}~$，202328020629002
    }
    \IEEEauthorblockA{
       电子邮箱地址，如 songchao2023@ia.ac.cn \\
       \textit{$^{\dagger}$中国科学院自动化研究所}\\
       \textit{$^{\ddagger}$中国科学院大学}
    }
}

\maketitle

\begin{abstract}
	本文通过论文\textit{Extreme Learning Machine: Theory and Applications}\cite{elm:huang}及相关代码的研读，学习并描述了ELM算法的理论依据、实现方法以及各方面的性能表现，然后将其应用在\textit{datafountain.cn}中举办的“返乡人群发展预测”的数据分析与建模比赛中，赛题基于中国联通的大数据能力，通过使用对联通的信令数据、通话数据、互联网行为等数据进行建模，对个人是否会返乡工作进行判断。在此问题中，我们简单应用了ELM算法，取得好优良的效果，且具有进一步提升的空间。
	
	在过去的几十年中，人们认为前馈神经网络是因为学习速度不够快而限制了其广泛的应用。这背后主要有两个原因，一是被广泛用于训练神经网络的梯度的学习算法比较慢，二是对神经网络的所有参数进行迭代微调也需要使用基于梯度的学习算法。与传统实现不同，本文针对单隐层前馈神经网络(SLFNs)提出了一种新的学习算法——极限学习机(ELM)，该算法随机选择隐藏节点并解析确定SLFNs的输出权值。理论上，该算法可以在极快的学习速度下提供良好的泛化性能。基于几个人工和真实的基准函数逼近和分类问题(包括非常大的复杂应用)的实验结果表明，新算法在大多数情况下都具有良好的泛化性能，学习速度比传统的流行前馈神经网络学习算法快数千倍。
\end{abstract}


\section{简介}

前馈神经网络在许多领域得到了广泛的应用，因为它具有以下能力：
\begin{enumerate}
	\item 可以从输入样本学习到复杂的非线性映射；
	\item 在难以用经典参数技术处理的大规模模式识别问题中表现良好。
\end{enumerate}

但是，传统的学习算法在训练速度上通常难以令人满意，花费几个小时、几天甚至更多时间来训练一个模型是司空见惯的事情。

从数学的角度来看，前馈神经网络的逼近能力着重看两个方面：致密输入集上的全局逼近和有限训练样本集上的逼近。许多研究者探索了标准多层前馈神经网络的全局逼近能力。Hornik 曾证明，在致密输入集上使用神经网络可以在度量上逼近连续映射，只要激活函数是连续的、有界和非常数的。Leshno改进了Hornik的工作，证明了具有非多项式激活函数的前馈网络可以在度量上逼近连续函数。在实际应用中，神经网络是在有限的训练集上完成训练的，对于此类函数近似，Huang和Babri表明，使用N个隐藏节点和任一种非线性激活函数的单隐层前馈神经网络(SLFN)可以准确地学习N个不同的观测。在之前的理论研究工作以及几乎所有的前馈神经网络学习算法中，输入权值隐藏层偏置都是需要在训练中进行更新的。

传统上，前馈网络的所有参数都需要微调，因此不同层的参数(权重和偏置)之间存在依赖关系，在过去的几十年中，此类学习算法几乎都基于梯度下降方法，但是这类学习方法经常因为学习步骤设置得不合适而速度很慢，而且很容易收敛到局部最小值。为了获得更好的学习效果，这种学习算法通常需要许多迭代学习步骤。然而通过我们对人工和真实大型应用的一些模拟结果发现，输入权重和第一隐藏层偏置的更新并不是必要的，省去这些步骤不仅学习速度极快，泛化性能也很好。

在本文中，我们首先严格证明，只要隐藏层的激活函数是无限可微的，那么单隐层前馈神经网络的输入权重和隐藏层偏置可以随机分配，不必微调。然后单隐层前馈神经网络可以简单地被视为线性系统，其连接隐藏层和输出层的输出权重可以通过隐藏层输出矩阵的简单广义逆运算来分析确定。基于这一点，本文提出了名为“极端学习机（ELM）”的简单学习算法，其学习速度比传统的反向传播（BP）等前馈网络学习算法快数千倍，同时还能获得更好的泛化能力。Bartlett的关于前馈神经网络泛化能力的理论曾提到，最小化训练误差时，得到的权重范数越小，模型的泛化能力通常也更强。ELM算法就倾向于达到最小的训练误差、最小的权重范数，这正是其泛化能力取得良好表现的原因。因为这些不同于传统单隐层前馈神经网络的特点，我们为这种算法起名“极端学习机”。

本文的组织结构如下：第2部分严格证明，如果隐藏层的激活函数是无限可微的，则单隐层前馈神经网络（SLFNs）的输入权重和隐藏层偏置可以随机分配。第3部分详细描述使用单隐藏层前馈神经网络（SLFNs）的ELM学习算法。第4部分中给出性能评估。第5部分中给出一些讨论和结论。在算法中起着重要作用的\textit{Moore-Penrose}广义逆变换和通用线性系统的最小化最小二乘解范数等内容，将在附录中简要介绍。


\section{使用随机隐节点的单隐层前馈网络}

对于 $N$ 个完全不同的样本 $({\bf x}_i, {\bf t})i$，其中 ${\bf x}_i=[x_{i1},x_{i2},\dots,x_{in}]^T\in{\bf R}^n, {\bf t}_i=[t_{i1},t_{i2},\dots,t_{im}]^T\in{\bf R}^m$，拥有 $\tilde N$ 个隐节点的 SLFNs 并使用激活函数 g(x) 的模型可表达为如下形式：
\begin{equation}\label{eq1}
	\begin{split}
		\overset{\tilde{N}}{\underset{i=1}{\sum}}{{\bm{\beta}}_i}{g_i}({\bf{x}}_j)&=\overset{\tilde{N}}{\underset{i=1}{\sum}}{{\bm{\beta}}_i}g({{\bf{w}}_i}\cdot{{\bf{x}}_j}+b_i)={\bf{o}}_j\\
		&\quad j=1,\dots,N
	\end{split}
\end{equation}
其中 $ {\bf{w}}_i=[w_{i1},w_{i2},\dots,w_{in}]^T $ 是连接第 $i$ 个隐节点与输入节点的权重向量，$ {\bm{\beta}}_i=[\beta_{i1},\beta_{i2},\dots,\beta_{im}]^T $ 是连接第 $i$ 个隐节点与输出节点的权重向量；$b_i$是第$i$个隐节点的阈值。${{\bf{w}}_i}\cdot{{\bf{x}}_j}$表示${{\bf{w}}_i}$和${{\bf{x}}_j}$的内积。

拥有 $\tilde N$ 个隐节点的 SLFNs 并使用激活函数 g(x) 的模型可以零误差逼近这 $N$ 个样本意味着 $\sum^{\tilde{N}}_{j=1}||{\bf{o}}_j-{\bf{t}}_j||=0$，举例来说，存在 $ {\bm{\beta}}_i$、${{\bf{w}}_i}$和$b_i$，使得
\begin{equation}\label{eq2}
	\overset{\tilde{N}}{\underset{i=1}{\sum}}{{\bm{\beta}}_i}g({{\bf{w}}_i}\cdot{{\bf{x}}_j}+b_i)={\bf{t}}_j,\hspace{2em}j=1,\dots,N.
\end{equation}
这$N$个等式可以写成

\begin{equation}\label{eq3}
	{\bf{H}}\cdot{\bm{\beta}}={\bf{T}}
\end{equation}

其中
\begin{equation}
	\begin{split}\label{eq4}
		& {\bf{H}}({\bf{w}}_1,\dots,{\bf{w}}_{\tilde{N}},b_1,\dots,b_{\tilde{N}},{\bf{x}}_1,\dots,{\bf{x}}_{\tilde{N}})\\
		= & \begin{bmatrix}
			g({{\bf{w}}_1}\cdot{{\bf{x}}_1}+b_1) & \dots & g({{\bf{w}}_{\tilde{N}}}\cdot{{\bf{x}}_1}+b_{\tilde{N}}) \\
			\vdots & \dots & \vdots \\
			g({{\bf{w}}_1}\cdot{{\bf{x}}_N}+b_1) & \dots & g({{\bf{w}}_{\tilde{N}}}\cdot{{\bf{x}}_N}+b_{\tilde{N}})
		\end{bmatrix}_{N\times{\tilde{N}}}
	\end{split}
\end{equation}
\begin{equation}\label{eq5}
	{\bm{\beta}}=
	\begin{bmatrix}
		{\bm{\beta}}_1^T\\
		\vdots \\
		{\bm{\beta}}_{\tilde{N}}^T
	\end{bmatrix}_{\tilde{N}\times m},
	{\bf{T}}=\begin{bmatrix}
		{\bf{t}}_1^T\\
		\vdots\\
		{\bf{t}}_{\tilde{N}}^T
	\end{bmatrix}_{N\times m}
\end{equation}
这里 ${\bf{H}}$ 称为隐层输出矩阵，它的第$i$列是与输入${\bf x}_1,{\bf x}_2,\dots,{\bf x}_N$相对应的第$i$个隐节点的输出。
如果激活函数$g$是无限可微的，那么可以证明需要的节点数量$\tilde{N}\le N$。更严格的表述如下：

\textbf{定理2.1}\textit{给定一个具有$N$个隐节点的标准 SLFN 以及一个在任何区间都无限可微的激活函数 $g: R\rightarrow R$，对 $N$ 个完全不同的样本 $({\bf x}_i,{\bf t}_i )$，其中 ${\bf x}_i\in R^n$ 且 ${\bf t}_i\in R^m$，对分别在 $R^n$ 和 $R$ 任意区间上随机取得的${\bf w}_i$和$b_i$，不论依何种连续概率分布，都一定有 ${\bf H}$ 可逆且${||\bf{H}}\cdot{\bm{\beta}}-{\bf{T}}||=0$ 。}

\textbf{证明}\ 考虑一个向量 ${\bf c}(b_i)=[g({\bf x}_1),g({\bf x}_N)]^T=[g({\bf w}_i\cdot{\bf x}_1+b_i),g({\bf w}_i\cdot{\bf x}_N+b_i)]^T$，它表示在欧氏空间 $R^N$中 ${\bf H}$ 的第$i$列，而 $b_i\in(a,b)$ 且 $(a,b)$ 可以是 ${\bf R}$ 上的任意区间。基于前人的工作，我们可以用反证法证明 ${\bf c}$ 不属于维度小于 $N$ 的任意子空间。

又 ${\bf w}_i$ 随机生成于一个连续概率分布，可以假设对任意$k\ne k'$，${\bf w}_i\cdot {\bf x}_k \ne {\bf w}_i\cdot {\bf x}_k'$。如果 ${\bf c}$ 属于一个维度为 $N-1$ 的子空间，那么必定存在一个向量 ${\bm \alpha}$ 与这个子空间正交
\begin{equation}\label{eq6}
	\begin{split}
		&({\bm \alpha},{\bf c}(b_i)-{\bf c}(a))\\
		=&{\alpha}_1\cdot g(b_i+d_1)+{\alpha}_2\cdot g(b_i+d_2)\\
		&+\dots+{\alpha}_N\cdot g(b_i+d_N)-z\\
		=&0
	\end{split}
\end{equation}
此处 $d_k={\bf w}_i\cdot{\bf x}_k$，$k=1,\dots,N$ 以及 $z={\bm \alpha}\cdot{\bf c}(a), \forall b_i\in(a,b)$。假设 $\alpha_N\ne0$，此等式可进一步写为
\begin{equation}\label{eq7}
	g(b_i+d_N)=-\overset{N-1} {\underset {p=1}\sum}\gamma^p g(b_i+d_p)+z/\alpha_N
\end{equation}
此处有$\gamma_p=\alpha_p/\alpha_N, p=1,2,\dots,N-1$。又$g(x)$在任意区间上无限可微，可得
\begin{equation}\label{eq8}
	\begin{split}
		g^{(l)}(b_i+d_N)=-\overset{N-1} {\underset {p=1}\sum}\gamma^{(l)} g(b_i+d_p)\\
		l=1,2,\dots,N,N+1,\dots
	\end{split}
\end{equation}
$g^{(l)}$表示函数$g$关于$b_i$的第$l$阶微分。然而，对于生成的多于 $N-1$ 个的线性等式，仅有 $N-1$ 个自由系数 $\gamma_1,\gamma_2,\dots,\gamma_{N-1}$，这是不成立的。这样我们就证明了，向量${\bf c}$不属于任意一个维度少于 $N$ 的子空间。

由此可得，从任意区间$(a,b)$为$N$个隐节点随机取$N$个偏置值$b_1,\dots,\b_N$，有可能使得相应的向量组${\bf c}(b_1),{\bf c}(b_2),\dots,{\bf c}(b_N)$能够生成$R^N$。也就是说，在 $R^n$ 和 $R$ 任意区间上、按照任何一种连续概率分布，随机取得权重向量${\bf w}_i$和偏置值$b_i$，必然可使${\bf H}$的列向量满秩。$\square$

进一步的，我们有

\textbf{定理2.2}\textit{
	给定一个任意小的正数$\varepsilon>0$以及一个在任意定义区间上无限可微的激活函数$g: R\rightarrow R$，存在${\tilde{N}\le N}$，使得对 $N$ 个完全不同的样本 $({\bf x}_i,{\bf t}_i )$，其中 ${\bf x}_i\in R^n$ 且 ${\bf t}_i\in R^m$，对分别在 $R^n$ 和 $R$ 任意区间上随机取得的${\bf w}_i$和$b_i$，不论依何种连续概率分布，都一定有$||{\bf H}_{N\times \tilde{N}}\beta_{\tilde{N}\times m}-{\bf T}_{N\times m}||<\varepsilon$。
}

\textbf{证明}\ 可以使用反证法，若此定理不成立，根据定理2.1，我们可以直接令${\tilde{N}=N}$，使得$||{\bf H}_{N\times \tilde{N}}\beta_{\tilde{N}\times m}-{\bf T}_{N\times m}||<\varepsilon$。\square


\section{极限学习机ELM}

基于定理2.1和2.2，我们提出一种极其简单且高效的方法来训练单隐层前馈神经网络（SLFNs）。

\subsection{传统基于梯度的SLFNs}

按照传统方法，要训练一个 SLFN，需要找到特定的$\hat{\bf{w}}_i,\hat b_i,\hat{\bm{\beta}}(i=1,\dots,\tilde{N})$，使得
\begin{equation}\label{eq9}
	\begin{split}
		&||{\bf{H}}(\hat{\bf{w}}_1,\dots,\hat{\bf{w}}_{\tilde{N}},\hat b_1,\dots,\hat b_{\tilde{N}})\hat{\bm{\beta}}-{\bf{T}}||\\
		=&\underset{{\bf{w}}_i,b_i,{\bm \beta}}{min}||{\bf{H}}({\bf{w}}_1,\dots,{\bf{w}}_{\tilde{N}},b_1,\dots,b_{\tilde{N}}){\bm{\beta}}-{\bf{T}}||
	\end{split}
\end{equation}
求解此式等价于最小化如下代价函数
\begin{equation}\label{eq10}
	E=\sum_{j=1}^N(\sum_{i=1}^{\tilde{N}}\beta_i g({\bf{w}}_i\cdot{\bf{x}}_j+b_i)-t_j)^2
\end{equation}

当${\bf H}$未知时，对${\bf{H}}\cdot{\bm{\beta}}-{\bf{T}}$的最小值的搜索通常使用的是基于梯度的学习算法，在使用此类算法求最小值的过程中，由权重$({\bf w}_i, {\bm \beta}_i)$和偏置参数$(b_i)$组成的向量${\bf W}$由下式迭代更新
\begin{equation}\label{eq11}
	{\bf{W}}_k={\bf{W}}_{k-1}-\eta \frac{\partial E({\bf{W}})}{\partial {\bf{W}}}
\end{equation}
其中$\eta$表示学习率。在前馈神经网络中BP算法用得最多，它可以通过从输出到输入的传播来高效地计算梯度，但是它有几个问题：
\begin{enumerate}
	\item 当学习率$\eta$过小，学习算法收敛过慢。但是当$\eta$过大时，算法会变得不稳定并且发散；
	\item 误差曲面影响BP学习算法性能的另一个特性是局部最小值的存在。如果学习算法停在一个位于全局最小值之上的局部最小值处，这是不可取的；
	\item 使用BP算法训练神经网络可能会过训练，导致泛化性能变差。因此，在代价函数最小化过程中需要验证和适当的停止方法；
	\item 基于梯度的学习在大多数应用中非常耗时。
\end{enumerate}

而本文的目的就在于解决上述问题，并针对前馈神经网络提出一种新而且有效的学习算法。

\subsection{用于SLFN的最小化范数最小二乘法}

基于前面的定理2.1和2.2，如果激活函数是无限可微的，则输入权重和隐藏层偏置可以随机分配。
与传统方法中“SLFN的所有参数都需要调整”这点认识不同，我们认为输入权重${\bf w}_i$和隐藏层偏置$b_i$并不一定需要调整，在训练开始时为这些参数随机赋值之后，隐藏层输出矩阵${\bf H}$可以一直保持不变。我们让输入权重 ${\bf w}_i$ 和隐层偏置$b_i$保持不变，根据公式\eqref{eq9}，训练一个SLFN可以简化为求线性方程${\bf{H}}\cdot{\bm{\beta}}={\bf{T}}$的最小二乘解$\hat{\bm{\beta}}$：
\begin{equation}\label{eq12}
	\begin{split}
		&||{\bf{H}}({\bf{w}}_1,\dots,{\bf{w}}_{\tilde{N}},b_1,\dots,b_{\tilde{N}})\hat{\bm{\beta}}-{\bf{T}}||\\
		=&\underset{\bm \beta}{min}||{\bf{H}}({\bf{w}}_1,\dots,{\bf{w}}_{\tilde{N}},b_1,\dots,b_{\tilde{N}}){\bm \beta}-{\bf{T}}||
	\end{split}
\end{equation}
如果隐节点的数量$\tilde{N}$跟特异样本的数量$N$相同，即$\tilde{N}=N$，当输入权重向量${\bf w}_i$和隐藏偏置$b_i$是随机生成的时候，${\bf H}$是一个方阵且可逆，此时SLFN可以以0误差逼近这些训练样本。

然而，多数情况下，隐节点的数量要远少于特异样本的数量，即$\tilde{N}\ll N$，此时$\bf H$并不是一个方阵且使得${\bf{H}}\cdot{\bm{\beta}}={\bf{T}}$成立的${\bf{w}_i},b_i,{\bm{\beta}} (i=1,\dots,{\tilde{N}})$也不存在。基于一些前人的工作，上述线性方程组的最小范数最小二乘解为
\begin{equation}\label{eq13}
	\hat {\bm \beta}={\bf H}^{\dagger}{\bf T}
\end{equation}
这里的${\bf H}^{\dagger}$表示矩阵${\bf H}$的\textit{Moore–Penrose}广义逆矩阵。

\textbf{备注 1}\ 经过分析，ELM具有以下性质：
\begin{enumerate}
	\item \textit{最小训练误差}\ 特解$\hat {\bm \beta}={\bf H}^{\dagger}{\bf T}$是广义线性方程组${\bf{H}}\cdot{\bm{\beta}}={\bf{T}}$的一个最小二乘解，意味着最小训练误差可以通过下面的特解得到：
	\begin{equation}\label{eq14}
		||{\bf H}{\hat {\bm \beta}}-{\bf T}||=||{\bf H}{\bf H}^{\dagger}{\bf T}-{\bf T}||=\underset{\bm \beta}{min}||{\bf H}{\bm \beta}-{\bf T}||
	\end{equation}
	纵使大部分学习算法都希望得到这个最小误差，但大部分算法都会因为局部最小值问题或者不可能无限大的迭代次数而无法真正得到这个最小误差。
	\item \textit{权重最小范数}\ 进一步来说，特解$\hat {\bm \beta}={\bf H}^{\dagger}{\bf T}$是广义线性方程组${\bf{H}}\cdot{\bm{\beta}}={\bf{T}}$所有最小二乘解中范数最小的解：
	\begin{equation}\label{eq15}
		\begin{split}
			&||\hat {\bm \beta}||=||{\bf H}^{\dagger}{\bf T}||<||{\bm\beta}||,\ \forall {\bm\beta}\in\\
			&\{{\bm\beta}:||{\bf H}{\bm\beta}-{\bf T}||\le{\bf H}{\bf z}-{\bf T}, \forall{\bf z}\in{\bf R}^{{\tilde N}\times N}\}
		\end{split}
	\end{equation}
	\item $\hat {\bm \beta}={\bf H}^{\dagger}{\bf T}$是广义线性方程组${\bf{H}}\cdot{\bm{\beta}}={\bf{T}}$的惟一最小范数最小二乘解。
\end{enumerate}

\subsection{用于SLFN的学习算法}

此时，一个我们称之为“极限学习机ELM”的简单SLFN学习算法可以总结如下：

\textbf{ELM 算法}\ 给定一个训练集$\aleph=\{({\bf x}_i,{\bf t}_i)\vert {\bf x}_i\in R^n,{\bf t}_i\in R^m,i=1,\dots,N\}$，激活函数$g(x)$，以及隐节点数量$\tilde N$，

\textit{步骤 1}\ 随机为权重${\bf w}_i$和偏置$b_i$赋值，其中$i=1,\dots,{\tilde N}$；

\textit{步骤 2}\ 计算隐层输出矩阵{\bf H}；

\textit{步骤 3}\ 计算输出权重$\bm \beta$：
	\begin{equation}\label{eq16}
		{\bm \beta}={\bf H}^{\dagger}{\bf T}，
	\end{equation}
	其中${\bf T}=[{\bf t}_1,\dots,{\bf t}_N]^T$。

\textbf{备注 2}\ 从定理2.1中可以看出，此算法适用于任何无限可微的激活函数$g(x)$，包括 s 形曲线函数、正弦、余弦函数、径向基函数以及一些非常规函数。根据定理2.1，所需要的隐节点的数量上限就是特异样本的数量，亦即$\tilde N \le N$。

\textbf{备注 3}\ 一些他人的工作表明，带有$N$个隐节点的SLFN恰好可以区分$N$种不同的样本，然而我们证明，输入权重与隐层偏置如果是随机生成的，此结论仍然成立。我们还证明，在允许一定学习误差的情况下，SLFN所需要的隐节点数量可以更少。其它人的工作都需要调整输入权重和隐层偏置，或者输入权重随机生成但隐层偏置需要调整，而ELM是输入权重和隐层偏置都是随机生成的。

\textbf{备注 4}\ 在其它一些工作中，也有人提出使用模块化网络，这类方法的思想是把训练样本划分为$L$个子集，然后分别使用$L$个不同的SLFN进行学习。记第$i$个子集对应的SLFN中的隐节点数量为$s_i$，这时我们应用ELM的话，会发现不同模块对应的SLFN中的隐节点实际上是趋同的，也就是说，在某个SLFN中第$i$个隐节点放到其它任意一个SLFN中的第$i$个隐节点的位置，模型仍然可以正常工作，隐节点所需要的总数量可取$max_i(s_i)$。在其它人的工作中，如果仅输出权重是随机生成的，隐层偏置需要调整的话，这样的方法则没有上述特性。

\textbf{备注 5}\ 有几种不同的方法可以计算矩阵${\bf H}$的\textit{Moore–Penrose}广义逆矩阵。这些方法包括但不限于正交投影、正交化方法、迭代方法以及单值分解法（SVD）。其中正交化方法和迭代法里使用了查询和迭代，这是我们不想在ELM中使用的；正交投影方法在${\bf H}^T{\bf H}$非奇异且${\bf H}^{\dagger}=({\bf H}^T{\bf H})^{-1}{\bf H}^T$时可以使用，但是${\bf H}^T{\bf H}$并非总是非奇异的，所以这种方法也总是有效。只有SVD方法在任何情况下都可以用来计算${\bf H}$的\textit{Moore–Penrose}广义逆矩阵。

\section{性能评估}

我们将ELM算法与其它用于SLFN的流行算法（比如BP算法和支持向量机）在函数逼近与分类等领域的几个基准上进行比较。所有BP和ELM算法都在MATLAB6.5环境中执行，CPU为奔腾4（1.9GHz）。虽然BP算法有很多变体，我们这里采用的是\textit{Levenberg-Marquardt}算法，在MATLAB工具中有此方法的实现。SVM算法使用的是LIBSVM4，通过源代码编译而来，也在同一台计算机上执行。SVM使用的核函数是径向基函数，我们在ELM算法中使用的激活函数是$g(x)=1/(1+exp(-x))$。在数据处理上，所有的输入都初归一化到0到1的范围内，输出则被归一化到-1到1的范围内。在ELM算法中，主要的学习时间花费在计算隐层输出矩阵${\bf H}$的\textit{Moore-Penrose}广义逆上。

\subsection{回归类问题基准}

\subsubsection{手工成生数据-拟合带噪声的SinC函数}

这里我们使用三种方法——BP算法、SVR算法和ELM算法——来似合SinC函数，这个函数通常用来验证SVR的有效性：
\begin{empheq}[left={\eqmath[r]{A}{y(x)=}\empheqlbrace}]{alignat=2}
	\begin{aligned}
		& sin(x)/x, & x\ne0,\\
		& 1, & x=0.
	\end{aligned}
\end{empheq}
训练集$(x_i,y_i)$和测试集$(x_i,y_i)$共有5000条数据，其中$x_i$随机取自$(-10,10)$上的均匀分布。为了尽可能模拟真实情况，我们在训练集上添加了位于$[-0.2,0,2]$上均匀分布的噪声；测试集不添加噪声。

在ELM算法和BP算法中，我们使用相同的隐节点数20。在测试中，在得到类似RSME的时候，ELM训练速度比BP快170倍。相比SVR，ELM的训练速度更是快了1000倍。

\subsubsection{现实世界中的回归问题}

这里我们在13个真实世界的基准数据集上对ELM、BP和SVR算法进行比较，在这些问题中，数据分布是未知的，而且其中大多数都含有噪声。对每一次测试，使用的训练和测试数据，都是基于整个数据集重新随机产生的。对ELM和BP算法而言，我们使用交叉验证方法以5为单位逐步地增加隐节点的数量，最终两个方法会得到数量接近的优化节点数。这里我们重复执行了50次实验，对ELM和BP算法分别取其中最好的结果来作比。从结果来看，ELM和SVR表现出了更好的泛化能力，在很多情况下比BP算法好一点点。ELM需要的隐节点数量似乎比BP算法要多一点，但是比SVR方法要紧致得多。在所有的测试中，ELM都是训练最快的，它比BP算法要快上几百倍。然而是测试的时候，BP的测试速度是最快的，这是因为BP模型通常拥有最紧凑的网络结构。另外，ELM算法的隐节点数量在很大范围内变化时，泛化能力可以一直保持稳定，只有在隐节点数量过少的时候，其泛化能力才开始有所下降。

\subsection{中小规则分类问题上的基准测试}

\subsubsection{关于糖尿病的医疗诊断}

这里使用了由约翰霍普金斯大学应用物理实验室于1988年提供的\textit{Pima Indians Diabetes Database}，来比较ELM与其它一些流行算法之间的性能差异。这种诊断方法使用WHO的标准来对病人是否患有糖尿病作出一个二元判断。使用20个隐节点的ELM算法得到了77.57\%的准确率，略高于其它方法；而在学习速度上更是比BP算法快了约300倍，比SVM算法快了约15倍。

\subsubsection{在中等规模分类问题上的应用}

我们也在\textit{Banana database}和\textit{Statlog}里的一些多分类数据集（包括\textit{SatImage}、\textit{Segment}和班机降落控制数据库）上对ELM进行了测试。因\textit{Banana database}训练数据中存在大量重复数据，我们在进行训练的时候执行了去重操作，只保留了40,000条数据中的5200条数据。在训练结果不逊于甚至超过BP算法的情况下，ELM算法的训练速度达到了BP算法的4000多倍。

\subsection{现实世界超大规模复杂问题上的基准测试}

这里采用了美国森林服务区域2资源信息系统的数据，共有581,012条数据，每条数据有54个属性，我们将其划分为100,000条训练数据与481,012条测试数据。我们用ELM算法执行了50次测试，SVM算法实在太慢了，所以我们只测试了一次。ELM算法在获得了比SVM更好的泛化能力的情况下，只用了1.6分钟就完成训练；而SVM却用了12个小时之久；在测试集上，ELM算法消耗的时间尚不到1分钟，而SVM却使用了5.5个小时。BP算法则因为资源需求问题无法完成训练，然而从其它研究人员的报告来看，即使完成了训练，其结果准确度也比ELM算法要低9\%左右。

\section{分析与结论}

本文提出了一种简单且高效的基于SLFN的算法ELM，与其它流行的类似算法相比，它表现出了很多与众不同的特性：
\begin{enumerate}
	\item 训练速度极快。在同样的运行环境下，相比BP算法和SVM算法，ELM算法在很多基准测试中比其它算法要快上百倍甚至几千倍；
	\item 在多数场景中，ELM算法的泛化能力都超过了基于后向传播的梯度算法；
	\item 传统的基于梯度的训练方法都面临着局部最优、不合适的学习率以及过拟合等一系列问题，此时不得不采用一些额外的手段来解决此类问题。相比之下，ELM方法则来得更加简单且直接，不需要这些迂回方法；
	\item 传统基于梯度的方法要求激活函数必须是可微的，但是ELM在很多不可微的激活函数上依然可以工作得很好。
\end{enumerate}

不过需要明确的是，基于梯度的后向传播优化算法适用于多隐层神经网络，但是ELM算法目前只支持单隐层神经网络，但是在理论上，ELM算法也可以支持多隐层神经网络。由此可见，ELM算法的可应用场景还是很多的。

另外，虽然SVM并非神经网络算法，但是作为一种经过长时间讨论和研究并充分优化和应用过的算法，ELM在性能不输的情况下分类速度比SVM快出了几千倍，这个对比结果非常能体现出ELM的优势。

当然，ELM还存在一些需要进一步探索的问题，比如ELM的全局逼近能力，以及它在稀疏高维数据上的应用。

\section{在“返乡发展人群预测”中的应用}

在ELM的官方主页\cite{ol:elmorg}中提供了一些ELM的实现代码，我们选择了其中的Python-ELM实现来完成对数据集的初步分析。相关代码\footnote{https://github.com/LeetJoe/Python-ELM}存放在托管网站\testit{GITHUB}上，随着后续工作进展，仓库内容可能会更新。

样本数据集共有199,540条数据，其中70\%是训练数据，包括一部分无标签数据；剩余30\%是测试数据。数据维度包括：
\begin{enumerate}
	\item 位置类特特征：基于联通基站产生的用户信令数据；
	\item 互联网类特征：基于联通用户上网产生的上网行为数据；
	\item 通话类特征：基于联通用户日常通话、短信产生的数据。
\end{enumerate}
除去id和label，共22个特征维度。在执行训练之前，我们将所有数据维度进行了归一化处理；对非数值的特征，我们将其进行分类然后赋值到实数域中。

\textbf{实验环境说明}\ 硬件环境为CPU intel i9-129000，内存32G；操作系统环境为Ubuntu22.04；代码编译及运行环境为Python3.10.13。

在一系列初步测试中，我们发现sigmoid的训练准确率更高一些，因此激活函数我们使用sigmoid函数，隐节点数量取$N=2000$。在未进行任何特征选择与提取的情况下，在训练集上得到了约85\%的准确率。使用这个模型，我们对训练数据应用了ROC曲线下面积AUC分析，成功识别出了约训练集中占比约10\%的干扰数据。排除干扰数据后，在训练集上的训练准确率超过了89\%。

得益于ELM的高效，我们可以快速对数据完成训练并获得优良的训练准确度，对我们进一步做特征的选择与提取提供了便利。

进一步的，我们分析了单层ELM的节点数与其在训练集上准确率的表现，并记录如下：

\begin{table}[h!t]
	\center
	\caption{隐节点数量与训练集准确率}
	\label{tab:1}
	\begin{tabular}{p{80pt}<{\centering}p{60pt}<{\centering}p{60pt}<{\raggedleft}}
		\hline\\[-2.9mm]\hline
		节点数量 & 准确率\% & 耗时(s) \\
		\hline
		1000 & 87.00 & 4.71\\
		2000 & 87.66 & 14.31\\
		3000 & 88.14 & 41.73\\
		4000 & 88.54 & 75.53\\
		5000 & 89.11 & 132.83\\
		6000 & 89.54 & 192.33\\
		7000 & 90.05 & 294.37\\
		8000 & 90.38 & 380.13\\
		9000 & 90.77 & 513.26\\
		10000 & 91.16 & 600.02\\
		\hline\\[-2.9mm]\hline
	\end{tabular}
\end{table}

通过表格\ref{tab:1}可以看出，ELM随着隐节点的数量增加，其在训练集上的准确率也不断有所提升，当然也伴随着一些时间消耗的增加。不过在最终测试中我们发现，当节点数量超过一定值时，其在测试集上的表现反而开始下降，也就是说隐节点数量过多的情况下，ELM也会出现过拟合的情况。

随后我们对训练数据执行了特征工程，使用相关性检测和离群点分析去掉一些相关性极低的维度和一部分离群点，执行训练之后发现准确度并没有明显的提升。由此似乎可以看出，ELM算法本身对离群点和无关维度就拥有比较强的区分能力，这也反映了ELM算法所具有的鲁棒性。

\textbf{进一步的工作}\ 初步结果显示出了ELM的优势，但是总体结果尚未达到历史最高准确率，通过分析前人的工作，我认为，这并非ELM本身的问题，而是需要在特征工程上对特征进行进一步的处理，比如多维组合特征、特征变换等，相信通过进一步的特征工程，训练准确率会不断提升，时间与篇幅限制，这些工作我将在后续完成。

\bibliography{sctest}
\bibliographystyle{IEEETran}

\end{document}














